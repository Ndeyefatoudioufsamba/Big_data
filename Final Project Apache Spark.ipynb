{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3532250-894b-455b-b7a3-e783dc1f0c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nom Etudiant :SAMBA  \n",
    "\n",
    "**Prenom Etudiant:Ndeye Fatou Diouf  \n",
    "\n",
    "**Classe :ING3 INFO \n",
    "\n",
    "**Date limite de dépot : 26/05/2023 avant 23h 59**\n",
    "\n",
    "**Merci de partagé le projet dans un repos Git public**\n",
    "\n",
    "\n",
    "# Travail à Faire:\n",
    "Télécharger le Datasets sur le lien Drive : https://drive.google.com/drive/folders/1DeciYELjrEoAWxahJf00rf1NHFPsiiYZ?usp=share_link  \n",
    "\n",
    "Repondre les questions ci-dessous avec le maximum de precisions et de détails.   \n",
    "Remplir `FILL_IN` avec les methodes qui correspondent à la réponse adéquate\n",
    "\n",
    "### Revenus des achats\n",
    "1. Extraire les revenus d'achat pour chaque événement\n",
    "2. Filtrer les événements dont le revenu n'est pas nul\n",
    "3. Vérifiez quels sont les types d'événements qui génèrent des revenus\n",
    "4. Supprimez la colonne inutile\n",
    "\n",
    "### Revenus par Traffic  \n",
    "Obtenir les 3 sources de trafic générant le revenu total le plus élevé.  \n",
    "5. Revenus cumulés par source de trafic  \n",
    "7. Obtenir les 3 principales sources de trafic par revenu total  \n",
    "6. Nettoyer les colonnes de revenus pour avoir deux décimales  \n",
    "8. Sauvegarder les données  \n",
    "\n",
    "### Obtenir les utilisateurs les plus actifs (active_users) par jour\n",
    "\n",
    "### Obtenir le nombre moyen d'utilisateurs actifs par jour de la semaine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Methods\n",
    "- DataFrame: `select`, `drop`, `withColumn`, `filter`, `dropDuplicates`,  `groupBy`, `sort`, `limit`\n",
    "- Column: `isNotNull`, `alias`, `desc`, `cast`, `operators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "124f66d5-a758-40f9-9254-9802175a7eee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.0.2.15:4040\n",
       "SparkContext available as 'sc' (version = 3.3.1, master = local[*], app id = local-1685482106014)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/30 21:28:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4dadf8b\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.\n",
    "                        appName(\"Projet Spark\").\n",
    "                        config(\"spark.ui.port\", \"0\").\n",
    "                        master(\"local[*]\").\n",
    "                        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e6a5343-3c8d-4b9b-9ea7-4b5897ff133b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eventsPath: String = datasets/events/events.json\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eventsPath = \"datasets/events/events.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54dcf1b1-4a15-48c7-a3bf-590499529d4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\n",
      "| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\n",
      "|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|      {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500|\n",
      "|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|   {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357|\n",
      "|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|       {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547|\n",
      "|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|       {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|\n",
      "|Windows|{null, null, null}|mattresses|                    null|1593878628143633| {Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108|\n",
      "|Windows|{null, null, null}|      main|                    null|1593878634344194|        {Medina, MN}|                  []|       youtube|          1593878634344194|UA000000107377161|\n",
      "|    iOS|{null, null, null}|      main|                    null|1593877936171803|{Mount Pleasant, UT}|                  []|        direct|          1593877936171803|UA000000107370851|\n",
      "|  macOS|{null, null, null}|      main|                    null|1593876843215329|      {Piedmont, AL}|                  []|     instagram|          1593876843215329|UA000000107360961|\n",
      "|Android|{null, null, null}|  warranty|        1593878529774474|1593879213196400|{Rancho Santa Mar...|                  []|     instagram|          1593878529774474|UA000000107376205|\n",
      "|Windows|{null, null, null}|      main|                    null|1593876713246514|        {Elyria, OH}|                  []|      facebook|          1593876713246514|UA000000107359805|\n",
      "|    iOS|{null, null, null}|  original|        1593878068949001|1593878170903989|      {Longview, WA}|                  []|        google|          1593877826716812|UA000000107369909|\n",
      "|  Linux|{null, null, null}|      main|                    null|1593878036347579|     {Lyndhurst, OH}|                  []|        direct|          1593878036347579|UA000000107371743|\n",
      "|Android|{null, null, null}|      down|        1593879057792999|1593879125815755|       {Jackson, MO}|                  []|      facebook|          1593879057792999|UA000000107380961|\n",
      "|  Linux|{null, null, null}|      main|                    null|1593878672173087|  {Cedar Rapids, IA}|                  []|        google|          1593878672173087|UA000000107377487|\n",
      "|  macOS|{null, null, null}|      main|                    null|1593876429415452|       {Phoenix, AZ}|                  []|        google|          1593876429415452|UA000000107357350|\n",
      "|    iOS|{null, null, null}|mattresses|                    null|1593876687337581|       {Warwick, RI}|                  []|        google|          1593876687337581|UA000000107359573|\n",
      "|  macOS|{null, null, null}|   premium|        1593877223736871|1593877973962436|       {Everett, WA}|                  []|     instagram|          1593877223736871|UA000000107364368|\n",
      "|Windows|{null, null, null}|   reviews|        1593876442432487|1593876944661570|       {Concord, CA}|                  []|        direct|          1593876442432487|UA000000107357467|\n",
      "|    iOS|{null, null, null}|  original|        1593877781854634|1593877788141768|      {Dunwoody, GA}|                  []|        google|          1593877781854634|UA000000107369512|\n",
      "|    iOS|{null, null, null}|      main|        1593877445670953|1593877497207417|     {Rochester, MN}|                  []|      facebook|          1593877300577217|UA000000107365065|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eventsDF: org.apache.spark.sql.DataFrame = [device: string, ecommerce: struct<purchase_revenue_in_usd: double, total_item_quantity: bigint ... 1 more field> ... 8 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eventsDF = spark.read.json(eventsPath)\n",
    "eventsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca867dd2-4e12-40ad-a63b-4300fe236849",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Extraire les revenus d'achat pour chaque événement\n",
    "Ajouter une nouvelle colonne **`revenue`** en faisant l'extration de **`ecommerce.purchase_revenue_in_usd`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b6760a2-7b47-4713-aed3-65db8157dbe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|               items|traffic_source|user_first_touch_timestamp|          user_id|revenue|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|      {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500|   null|\n",
      "|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|   {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357|   null|\n",
      "|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|       {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547|   null|\n",
      "|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|       {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|   null|\n",
      "|Windows|{null, null, null}|mattresses|                    null|1593878628143633| {Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108|   null|\n",
      "|Windows|{null, null, null}|      main|                    null|1593878634344194|        {Medina, MN}|                  []|       youtube|          1593878634344194|UA000000107377161|   null|\n",
      "|    iOS|{null, null, null}|      main|                    null|1593877936171803|{Mount Pleasant, UT}|                  []|        direct|          1593877936171803|UA000000107370851|   null|\n",
      "|  macOS|{null, null, null}|      main|                    null|1593876843215329|      {Piedmont, AL}|                  []|     instagram|          1593876843215329|UA000000107360961|   null|\n",
      "|Android|{null, null, null}|  warranty|        1593878529774474|1593879213196400|{Rancho Santa Mar...|                  []|     instagram|          1593878529774474|UA000000107376205|   null|\n",
      "|Windows|{null, null, null}|      main|                    null|1593876713246514|        {Elyria, OH}|                  []|      facebook|          1593876713246514|UA000000107359805|   null|\n",
      "|    iOS|{null, null, null}|  original|        1593878068949001|1593878170903989|      {Longview, WA}|                  []|        google|          1593877826716812|UA000000107369909|   null|\n",
      "|  Linux|{null, null, null}|      main|                    null|1593878036347579|     {Lyndhurst, OH}|                  []|        direct|          1593878036347579|UA000000107371743|   null|\n",
      "|Android|{null, null, null}|      down|        1593879057792999|1593879125815755|       {Jackson, MO}|                  []|      facebook|          1593879057792999|UA000000107380961|   null|\n",
      "|  Linux|{null, null, null}|      main|                    null|1593878672173087|  {Cedar Rapids, IA}|                  []|        google|          1593878672173087|UA000000107377487|   null|\n",
      "|  macOS|{null, null, null}|      main|                    null|1593876429415452|       {Phoenix, AZ}|                  []|        google|          1593876429415452|UA000000107357350|   null|\n",
      "|    iOS|{null, null, null}|mattresses|                    null|1593876687337581|       {Warwick, RI}|                  []|        google|          1593876687337581|UA000000107359573|   null|\n",
      "|  macOS|{null, null, null}|   premium|        1593877223736871|1593877973962436|       {Everett, WA}|                  []|     instagram|          1593877223736871|UA000000107364368|   null|\n",
      "|Windows|{null, null, null}|   reviews|        1593876442432487|1593876944661570|       {Concord, CA}|                  []|        direct|          1593876442432487|UA000000107357467|   null|\n",
      "|    iOS|{null, null, null}|  original|        1593877781854634|1593877788141768|      {Dunwoody, GA}|                  []|        google|          1593877781854634|UA000000107369512|   null|\n",
      "|    iOS|{null, null, null}|      main|        1593877445670953|1593877497207417|     {Rochester, MN}|                  []|      facebook|          1593877300577217|UA000000107365065|   null|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "revenueDF: org.apache.spark.sql.DataFrame = [device: string, ecommerce: struct<purchase_revenue_in_usd: double, total_item_quantity: bigint ... 1 more field> ... 9 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "val revenueDF = eventsDF.withColumn(\"revenue\", col(\"ecommerce.purchase_revenue_in_usd\"))\n",
    "revenueDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b36255e-809d-4f37-bf29-14a84fb54dbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Filtrer les événements dont le revenu n'est pas null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3ab4d8d-3ba9-4efe-a0b2-f64a313c52a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|   device|     ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|               items|traffic_source|user_first_touch_timestamp|          user_id|revenue|\n",
      "+---------+--------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|    Linux|{1195.0, 1, 1}|  finalize|        1593878893766134|1593878897648871|       {Shawnee, KS}|[{null, M_STAN_K,...|        google|          1593876996316576|UA000000107362263| 1195.0|\n",
      "|      iOS|{1045.0, 1, 1}|  finalize|        1593878485345763|1593878487460247|       {Detroit, MI}|[{null, M_STAN_Q,...|      facebook|          1593877230282722|UA000000107364432| 1045.0|\n",
      "|  Android| {595.0, 1, 1}|  finalize|        1593877930076602|1593878966392505|  {East Chicago, IN}|[{null, M_STAN_T,...|        google|          1593876889575474|UA000000107361347|  595.0|\n",
      "|      iOS|{2290.0, 2, 2}|  finalize|        1593877650094042|1593877652106953|       {Warwick, RI}|[{null, M_PREM_F,...|        google|          1593876687337581|UA000000107359573| 2290.0|\n",
      "|    macOS| {945.0, 1, 1}|  finalize|        1593879151529456|1593879197837168|     {Boonville, MO}|[{null, M_STAN_F,...|      facebook|          1593878603312910|UA000000107376872|  945.0|\n",
      "|  Windows| {595.0, 1, 1}|  finalize|        1593877908876473|1593878020119079|       {Hampton, VA}|[{null, M_STAN_T,...|        google|          1593877033894464|UA000000107362622|  595.0|\n",
      "|  Android| {945.0, 1, 1}|  finalize|        1593878355764861|1593878641498265|{White Bear Lake,...|[{null, M_STAN_F,...|        direct|          1593877080764516|UA000000107363039|  945.0|\n",
      "|Chrome OS|{1095.0, 1, 1}|  finalize|        1593879073813036|1593879191730221|   {San Antonio, TX}|[{null, M_PREM_T,...|     instagram|          1593877153633764|UA000000107363715| 1095.0|\n",
      "|    macOS|{1045.0, 1, 1}|  finalize|        1593877425584678|1593877429621158|        {Searcy, AR}|[{null, M_STAN_Q,...|        direct|          1593876851338276|UA000000107361027| 1045.0|\n",
      "|      iOS|{1045.0, 1, 1}|  finalize|        1593878984623390|1593879046209960|     {Southport, IN}|[{null, M_STAN_Q,...|     instagram|          1593876574686487|UA000000107358614| 1045.0|\n",
      "|  Windows|{1045.0, 1, 1}|  finalize|        1593434694145790|1593434696316863|      {Columbus, OH}|[{null, M_STAN_Q,...|     instagram|          1593432259276789|UA000000106006620| 1045.0|\n",
      "|    macOS|{1640.0, 2, 2}|  finalize|        1593436589297426|1593436628314991|{St. Petersburg, FL}|[{null, M_STAN_T,...|      facebook|          1593433771174842|UA000000106011218| 1640.0|\n",
      "|Chrome OS| {654.0, 2, 2}|  finalize|        1593427561736870|1593428248499261|        {Farley, IA}|[{null, M_STAN_T,...|        google|          1593424867073851|UA000000105991167|  654.0|\n",
      "|  Windows|{1045.0, 1, 1}|  finalize|        1593388148852162|1593388365815897|       {Houston, TX}|[{null, M_STAN_Q,...|        direct|          1593383873231306|UA000000105957934| 1045.0|\n",
      "|  Windows|{1795.0, 1, 1}|  finalize|        1593399598968123|1593399696924475|        {Monroe, LA}|[{null, M_PREM_Q,...|      facebook|          1593398458152390|UA000000105978828| 1795.0|\n",
      "|  Android|{1195.0, 1, 1}|  finalize|        1593416200959556|1593416273461695|{Donaldsonville, LA}|[{null, M_STAN_K,...|        google|          1593415794401939|UA000000105983663| 1195.0|\n",
      "|      iOS|{1195.0, 1, 1}|  finalize|        1593394478371736|1593395580886846|    {Shreveport, LA}|[{null, M_STAN_K,...|        google|          1593388592209010|UA000000105968663| 1195.0|\n",
      "|      iOS|{1128.6, 2, 2}|  finalize|        1593763506976463|1593763527292050|     {San Ramon, CA}|[{NEWBED10, M_STA...|         email|          1593439553381667|UA000000106033948| 1128.6|\n",
      "|  Android|{1075.5, 1, 1}|  finalize|        1593529030162886|1593529604747277|     {La Vernia, TX}|[{NEWBED10, M_STA...|         email|          1593385351347346|UA000000105961848| 1075.5|\n",
      "|  Windows| {595.0, 1, 1}|  finalize|        1593439102031927|1593439865607689|        {Boston, MA}|[{null, M_STAN_T,...|      facebook|          1593435521455326|UA000000106017264|  595.0|\n",
      "+---------+--------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "purchasesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [device: string, ecommerce: struct<purchase_revenue_in_usd: double, total_item_quantity: bigint ... 1 more field> ... 9 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "val purchasesDF = revenueDF.filter(col(\"revenue\").isNotNull)\n",
    "purchasesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c7799f6-4668-47fe-ad11-2d95fb42f3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Vérifiez quels sont les types d'événements qui génèrent des revenus\n",
    "Trouvez des valeurs **`event_name`** uniques dans **`purchasesDF`**. Il y a deux façons de faire :\n",
    "- Sélectionnez \"event_name\" et recupérer les enregistrements distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9be33ae8-08e1-4f11-901a-d11f94d1a2a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|event_name|\n",
      "+----------+\n",
      "|  finalize|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distinctDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [event_name: string]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "val distinctDF = purchasesDF.select(\"event_name\").distinct()\n",
    "distinctDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da232f23-514c-4423-975f-20162fec0cb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Supprimez la colonne inutile\n",
    "\n",
    "Puisqu'il n'y a qu'un seul type d'événement, supprimez **`event_name`** de **`purchasesDF`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cff64c80-f082-408f-8ad2-50abd75d2f75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|   device|     ecommerce|event_previous_timestamp| event_timestamp|                 geo|               items|traffic_source|user_first_touch_timestamp|          user_id|revenue|\n",
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|    Linux|{1195.0, 1, 1}|        1593878893766134|1593878897648871|       {Shawnee, KS}|[{null, M_STAN_K,...|        google|          1593876996316576|UA000000107362263| 1195.0|\n",
      "|      iOS|{1045.0, 1, 1}|        1593878485345763|1593878487460247|       {Detroit, MI}|[{null, M_STAN_Q,...|      facebook|          1593877230282722|UA000000107364432| 1045.0|\n",
      "|  Android| {595.0, 1, 1}|        1593877930076602|1593878966392505|  {East Chicago, IN}|[{null, M_STAN_T,...|        google|          1593876889575474|UA000000107361347|  595.0|\n",
      "|      iOS|{2290.0, 2, 2}|        1593877650094042|1593877652106953|       {Warwick, RI}|[{null, M_PREM_F,...|        google|          1593876687337581|UA000000107359573| 2290.0|\n",
      "|    macOS| {945.0, 1, 1}|        1593879151529456|1593879197837168|     {Boonville, MO}|[{null, M_STAN_F,...|      facebook|          1593878603312910|UA000000107376872|  945.0|\n",
      "|  Windows| {595.0, 1, 1}|        1593877908876473|1593878020119079|       {Hampton, VA}|[{null, M_STAN_T,...|        google|          1593877033894464|UA000000107362622|  595.0|\n",
      "|  Android| {945.0, 1, 1}|        1593878355764861|1593878641498265|{White Bear Lake,...|[{null, M_STAN_F,...|        direct|          1593877080764516|UA000000107363039|  945.0|\n",
      "|Chrome OS|{1095.0, 1, 1}|        1593879073813036|1593879191730221|   {San Antonio, TX}|[{null, M_PREM_T,...|     instagram|          1593877153633764|UA000000107363715| 1095.0|\n",
      "|    macOS|{1045.0, 1, 1}|        1593877425584678|1593877429621158|        {Searcy, AR}|[{null, M_STAN_Q,...|        direct|          1593876851338276|UA000000107361027| 1045.0|\n",
      "|      iOS|{1045.0, 1, 1}|        1593878984623390|1593879046209960|     {Southport, IN}|[{null, M_STAN_Q,...|     instagram|          1593876574686487|UA000000107358614| 1045.0|\n",
      "|  Windows|{1045.0, 1, 1}|        1593434694145790|1593434696316863|      {Columbus, OH}|[{null, M_STAN_Q,...|     instagram|          1593432259276789|UA000000106006620| 1045.0|\n",
      "|    macOS|{1640.0, 2, 2}|        1593436589297426|1593436628314991|{St. Petersburg, FL}|[{null, M_STAN_T,...|      facebook|          1593433771174842|UA000000106011218| 1640.0|\n",
      "|Chrome OS| {654.0, 2, 2}|        1593427561736870|1593428248499261|        {Farley, IA}|[{null, M_STAN_T,...|        google|          1593424867073851|UA000000105991167|  654.0|\n",
      "|  Windows|{1045.0, 1, 1}|        1593388148852162|1593388365815897|       {Houston, TX}|[{null, M_STAN_Q,...|        direct|          1593383873231306|UA000000105957934| 1045.0|\n",
      "|  Windows|{1795.0, 1, 1}|        1593399598968123|1593399696924475|        {Monroe, LA}|[{null, M_PREM_Q,...|      facebook|          1593398458152390|UA000000105978828| 1795.0|\n",
      "|  Android|{1195.0, 1, 1}|        1593416200959556|1593416273461695|{Donaldsonville, LA}|[{null, M_STAN_K,...|        google|          1593415794401939|UA000000105983663| 1195.0|\n",
      "|      iOS|{1195.0, 1, 1}|        1593394478371736|1593395580886846|    {Shreveport, LA}|[{null, M_STAN_K,...|        google|          1593388592209010|UA000000105968663| 1195.0|\n",
      "|      iOS|{1128.6, 2, 2}|        1593763506976463|1593763527292050|     {San Ramon, CA}|[{NEWBED10, M_STA...|         email|          1593439553381667|UA000000106033948| 1128.6|\n",
      "|  Android|{1075.5, 1, 1}|        1593529030162886|1593529604747277|     {La Vernia, TX}|[{NEWBED10, M_STA...|         email|          1593385351347346|UA000000105961848| 1075.5|\n",
      "|  Windows| {595.0, 1, 1}|        1593439102031927|1593439865607689|        {Boston, MA}|[{null, M_STAN_T,...|      facebook|          1593435521455326|UA000000106017264|  595.0|\n",
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cleanDF: org.apache.spark.sql.DataFrame = [device: string, ecommerce: struct<purchase_revenue_in_usd: double, total_item_quantity: bigint ... 1 more field> ... 8 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "val cleanDF = purchasesDF.drop(\"event_name\")\n",
    "cleanDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c9ca35ef-7fa1-4259-b7cc-59a42a7e3fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Revenus cumulés par source de trafic\n",
    "- Obtenir la somme de **`revenue`** comme **`total_rev`**\n",
    "- Obtenir la moyenne de **`revenue`** comme **`avg_rev`**\n",
    "\n",
    "N'oubliez pas d'importer toutes les fonctions intégrées nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bd64d109-4513-48f9-b5b4-85f4ef6b1b0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|   device|     ecommerce|event_previous_timestamp| event_timestamp|                 geo|               items|traffic_source|user_first_touch_timestamp|          user_id|revenue|\n",
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|    Linux|{1195.0, 1, 1}|        1593878893766134|1593878897648871|       {Shawnee, KS}|[{null, M_STAN_K,...|        google|          1593876996316576|UA000000107362263| 1195.0|\n",
      "|      iOS|{1045.0, 1, 1}|        1593878485345763|1593878487460247|       {Detroit, MI}|[{null, M_STAN_Q,...|      facebook|          1593877230282722|UA000000107364432| 1045.0|\n",
      "|  Android| {595.0, 1, 1}|        1593877930076602|1593878966392505|  {East Chicago, IN}|[{null, M_STAN_T,...|        google|          1593876889575474|UA000000107361347|  595.0|\n",
      "|      iOS|{2290.0, 2, 2}|        1593877650094042|1593877652106953|       {Warwick, RI}|[{null, M_PREM_F,...|        google|          1593876687337581|UA000000107359573| 2290.0|\n",
      "|    macOS| {945.0, 1, 1}|        1593879151529456|1593879197837168|     {Boonville, MO}|[{null, M_STAN_F,...|      facebook|          1593878603312910|UA000000107376872|  945.0|\n",
      "|  Windows| {595.0, 1, 1}|        1593877908876473|1593878020119079|       {Hampton, VA}|[{null, M_STAN_T,...|        google|          1593877033894464|UA000000107362622|  595.0|\n",
      "|  Android| {945.0, 1, 1}|        1593878355764861|1593878641498265|{White Bear Lake,...|[{null, M_STAN_F,...|        direct|          1593877080764516|UA000000107363039|  945.0|\n",
      "|Chrome OS|{1095.0, 1, 1}|        1593879073813036|1593879191730221|   {San Antonio, TX}|[{null, M_PREM_T,...|     instagram|          1593877153633764|UA000000107363715| 1095.0|\n",
      "|    macOS|{1045.0, 1, 1}|        1593877425584678|1593877429621158|        {Searcy, AR}|[{null, M_STAN_Q,...|        direct|          1593876851338276|UA000000107361027| 1045.0|\n",
      "|      iOS|{1045.0, 1, 1}|        1593878984623390|1593879046209960|     {Southport, IN}|[{null, M_STAN_Q,...|     instagram|          1593876574686487|UA000000107358614| 1045.0|\n",
      "|  Windows|{1045.0, 1, 1}|        1593434694145790|1593434696316863|      {Columbus, OH}|[{null, M_STAN_Q,...|     instagram|          1593432259276789|UA000000106006620| 1045.0|\n",
      "|    macOS|{1640.0, 2, 2}|        1593436589297426|1593436628314991|{St. Petersburg, FL}|[{null, M_STAN_T,...|      facebook|          1593433771174842|UA000000106011218| 1640.0|\n",
      "|Chrome OS| {654.0, 2, 2}|        1593427561736870|1593428248499261|        {Farley, IA}|[{null, M_STAN_T,...|        google|          1593424867073851|UA000000105991167|  654.0|\n",
      "|  Windows|{1045.0, 1, 1}|        1593388148852162|1593388365815897|       {Houston, TX}|[{null, M_STAN_Q,...|        direct|          1593383873231306|UA000000105957934| 1045.0|\n",
      "|  Windows|{1795.0, 1, 1}|        1593399598968123|1593399696924475|        {Monroe, LA}|[{null, M_PREM_Q,...|      facebook|          1593398458152390|UA000000105978828| 1795.0|\n",
      "|  Android|{1195.0, 1, 1}|        1593416200959556|1593416273461695|{Donaldsonville, LA}|[{null, M_STAN_K,...|        google|          1593415794401939|UA000000105983663| 1195.0|\n",
      "|      iOS|{1195.0, 1, 1}|        1593394478371736|1593395580886846|    {Shreveport, LA}|[{null, M_STAN_K,...|        google|          1593388592209010|UA000000105968663| 1195.0|\n",
      "|      iOS|{1128.6, 2, 2}|        1593763506976463|1593763527292050|     {San Ramon, CA}|[{NEWBED10, M_STA...|         email|          1593439553381667|UA000000106033948| 1128.6|\n",
      "|  Android|{1075.5, 1, 1}|        1593529030162886|1593529604747277|     {La Vernia, TX}|[{NEWBED10, M_STA...|         email|          1593385351347346|UA000000105961848| 1075.5|\n",
      "|  Windows| {595.0, 1, 1}|        1593439102031927|1593439865607689|        {Boston, MA}|[{null, M_STAN_T,...|      facebook|          1593435521455326|UA000000106017264|  595.0|\n",
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trafficDF: org.apache.spark.sql.DataFrame = [device: string, ecommerce: struct<purchase_revenue_in_usd: double, total_item_quantity: bigint ... 1 more field> ... 8 more fields]\n",
       "total_rev: Double = 99152.4\n",
       "avg_rev: Double = 1011.7591836734694\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "val trafficDF = cleanDF\n",
    "\n",
    "val total_rev = trafficDF.select(sum(\"revenue\")).as(\"total_rev\").first().getDouble(0)\n",
    "\n",
    "\n",
    "val avg_rev = trafficDF.select(avg(\"revenue\")).as(\"avg_rev\").first().getDouble(0)\n",
    "\n",
    "trafficDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "752ce304-d2e1-47f5-acd0-d5a30183bf19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Obtenir les cinqs principales sources de trafic par revenu total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89de0a73-924d-4b55-8ed2-9b936dabc496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|   device|     ecommerce|event_previous_timestamp| event_timestamp|                 geo|               items|traffic_source|user_first_touch_timestamp|          user_id|revenue|\n",
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "|    Linux|{1195.0, 1, 1}|        1593878893766134|1593878897648871|       {Shawnee, KS}|[{null, M_STAN_K,...|        google|          1593876996316576|UA000000107362263| 1195.0|\n",
      "|      iOS|{1045.0, 1, 1}|        1593878485345763|1593878487460247|       {Detroit, MI}|[{null, M_STAN_Q,...|      facebook|          1593877230282722|UA000000107364432| 1045.0|\n",
      "|  Android| {595.0, 1, 1}|        1593877930076602|1593878966392505|  {East Chicago, IN}|[{null, M_STAN_T,...|        google|          1593876889575474|UA000000107361347|  595.0|\n",
      "|      iOS|{2290.0, 2, 2}|        1593877650094042|1593877652106953|       {Warwick, RI}|[{null, M_PREM_F,...|        google|          1593876687337581|UA000000107359573| 2290.0|\n",
      "|    macOS| {945.0, 1, 1}|        1593879151529456|1593879197837168|     {Boonville, MO}|[{null, M_STAN_F,...|      facebook|          1593878603312910|UA000000107376872|  945.0|\n",
      "|  Windows| {595.0, 1, 1}|        1593877908876473|1593878020119079|       {Hampton, VA}|[{null, M_STAN_T,...|        google|          1593877033894464|UA000000107362622|  595.0|\n",
      "|  Android| {945.0, 1, 1}|        1593878355764861|1593878641498265|{White Bear Lake,...|[{null, M_STAN_F,...|        direct|          1593877080764516|UA000000107363039|  945.0|\n",
      "|Chrome OS|{1095.0, 1, 1}|        1593879073813036|1593879191730221|   {San Antonio, TX}|[{null, M_PREM_T,...|     instagram|          1593877153633764|UA000000107363715| 1095.0|\n",
      "|    macOS|{1045.0, 1, 1}|        1593877425584678|1593877429621158|        {Searcy, AR}|[{null, M_STAN_Q,...|        direct|          1593876851338276|UA000000107361027| 1045.0|\n",
      "|      iOS|{1045.0, 1, 1}|        1593878984623390|1593879046209960|     {Southport, IN}|[{null, M_STAN_Q,...|     instagram|          1593876574686487|UA000000107358614| 1045.0|\n",
      "|  Windows|{1045.0, 1, 1}|        1593434694145790|1593434696316863|      {Columbus, OH}|[{null, M_STAN_Q,...|     instagram|          1593432259276789|UA000000106006620| 1045.0|\n",
      "|    macOS|{1640.0, 2, 2}|        1593436589297426|1593436628314991|{St. Petersburg, FL}|[{null, M_STAN_T,...|      facebook|          1593433771174842|UA000000106011218| 1640.0|\n",
      "|Chrome OS| {654.0, 2, 2}|        1593427561736870|1593428248499261|        {Farley, IA}|[{null, M_STAN_T,...|        google|          1593424867073851|UA000000105991167|  654.0|\n",
      "|  Windows|{1045.0, 1, 1}|        1593388148852162|1593388365815897|       {Houston, TX}|[{null, M_STAN_Q,...|        direct|          1593383873231306|UA000000105957934| 1045.0|\n",
      "|  Windows|{1795.0, 1, 1}|        1593399598968123|1593399696924475|        {Monroe, LA}|[{null, M_PREM_Q,...|      facebook|          1593398458152390|UA000000105978828| 1795.0|\n",
      "|  Android|{1195.0, 1, 1}|        1593416200959556|1593416273461695|{Donaldsonville, LA}|[{null, M_STAN_K,...|        google|          1593415794401939|UA000000105983663| 1195.0|\n",
      "|      iOS|{1195.0, 1, 1}|        1593394478371736|1593395580886846|    {Shreveport, LA}|[{null, M_STAN_K,...|        google|          1593388592209010|UA000000105968663| 1195.0|\n",
      "|      iOS|{1128.6, 2, 2}|        1593763506976463|1593763527292050|     {San Ramon, CA}|[{NEWBED10, M_STA...|         email|          1593439553381667|UA000000106033948| 1128.6|\n",
      "|  Android|{1075.5, 1, 1}|        1593529030162886|1593529604747277|     {La Vernia, TX}|[{NEWBED10, M_STA...|         email|          1593385351347346|UA000000105961848| 1075.5|\n",
      "|  Windows| {595.0, 1, 1}|        1593439102031927|1593439865607689|        {Boston, MA}|[{null, M_STAN_T,...|      facebook|          1593435521455326|UA000000106017264|  595.0|\n",
      "+---------+--------------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topTrafficDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [traffic_source: string, total_revenue: double]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//TODO\n",
    "val topTrafficDF = trafficDF.groupBy(\"traffic_source\")\n",
    "                            .agg(sum(\"revenue\").as(\"total_revenue\"))\n",
    "                            .orderBy(desc(\"total_revenue\"))\n",
    "                            .limit(5)\n",
    "\n",
    "trafficDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e46516b0-0637-47c4-a29d-58b32c8760db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 7. Limitez les colonnes de revenus à deux décimales pointés\n",
    "- Modifier les colonnes **`avg_rev`** et **`total_rev`** pour les convertir en des nombres avec deux décimales pointés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9660408e-252d-4b78-a6e6-e3028640f4b0",
     "showTitle": false,
     "title": ""
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+---------+\n",
      "|traffic_source|total_revenue|  avg_rev|\n",
      "+--------------+-------------+---------+\n",
      "|         email|      36935.4|36,935.40|\n",
      "|        google|      28936.0|28,936.00|\n",
      "|      facebook|      12952.0|12,952.00|\n",
      "|        direct|       9129.0| 9,129.00|\n",
      "|     instagram|       8160.0| 8,160.00|\n",
      "+--------------+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "finalDF: org.apache.spark.sql.DataFrame = [traffic_source: string, total_revenue: double ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "\n",
    "\n",
    "var finalDF = topTrafficDF.withColumn(\"avg_rev\", format_number(col(\"total_revenue\"), 2))\n",
    "                          \n",
    "\n",
    "finalDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|traffic_source|total_revenue|\n",
      "+--------------+-------------+\n",
      "|         email|    36,935.40|\n",
      "|        google|    28,936.00|\n",
      "|      facebook|    12,952.00|\n",
      "|        direct|     9,129.00|\n",
      "|     instagram|     8,160.00|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "finalDF: org.apache.spark.sql.DataFrame = [traffic_source: string, total_revenue: string]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "\n",
    "\n",
    "var finalDF = topTrafficDF.withColumn(\"total_revenue\", format_number(col(\"total_revenue\"), 2))\n",
    "                          \n",
    "\n",
    "finalDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d5a1355c-e00e-4d31-a7bc-0661051fcb97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 8. Sauvegarder les données \n",
    "Sauvegarder les données sous le format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a5515ca2-6f91-4337-b949-0a0a3ed713cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "// TODO\n",
    "finalDF.write.parquet(\"datsets/fireServiceParquet.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Obtenir les utilisateurs les plus actifs (active_users) par jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|  event_name|     active_users|\n",
      "+------------+-----------------+\n",
      "|    add_item|UA000000107363293|\n",
      "|     careers|UA000000107372811|\n",
      "|        cart|UA000000107365944|\n",
      "|     cc_info|UA000000106005418|\n",
      "|    checkout|UA000000105969485|\n",
      "|    delivery|UA000000107368184|\n",
      "|        down|UA000000105965828|\n",
      "|email_coupon|UA000000107364509|\n",
      "|         faq|UA000000107376511|\n",
      "|    finalize|UA000000106038329|\n",
      "|        foam|UA000000107377335|\n",
      "|       guest|UA000000107370317|\n",
      "|       login|UA000000107359160|\n",
      "|        main|UA000000107379420|\n",
      "|  mattresses|UA000000107362077|\n",
      "|    original|UA000000107365372|\n",
      "|     pillows|UA000000107366381|\n",
      "|     premium|UA000000107363587|\n",
      "|       press|UA000000107374069|\n",
      "|    register|UA000000106001316|\n",
      "+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "activeUsersDF: org.apache.spark.sql.DataFrame = [event_name: string, active_users: string]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "\n",
    "val activeUsersDF = eventsDF.groupBy(\"event_name\", \"user_id\")\n",
    "                            .agg(count(\"*\").as(\"activity_count\"))\n",
    "                            .orderBy(desc(\"activity_count\"))\n",
    "                            .groupBy(\"event_name\")\n",
    "                            .agg(first(\"user_id\").as(\"active_users\"))\n",
    "\n",
    "activeUsersDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Obtenir le nombre moyen d'utilisateurs actifs par jour de la semaine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|day_of_week|avg_active_users|\n",
      "+-----------+----------------+\n",
      "|       null|            null|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "activeDowDF: org.apache.spark.sql.DataFrame = [day_of_week: int, avg_active_users: double]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO\n",
    "\n",
    "val activeDowDF = activeUsersDF.groupBy(dayofweek(col(\"event_name\")).as(\"day_of_week\"))\n",
    "                               .agg(avg(\"active_users\").as(\"avg_active_users\"))\n",
    "\n",
    "activeDowDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 11. Sauvegarder les données \n",
    "Sauvegarder les données sous le format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/30 21:40:39 ERROR FileOutputCommitter: Mkdirs failed to create file:/datasets/fireServiceParquet.parquet /_temporary/0\n",
      "23/05/30 21:40:40 ERROR Executor: Exception in task 0.0 in stage 111.0 (TID 57)\n",
      "java.io.IOException: Mkdirs failed to create file:/datasets/fireServiceParquet.parquet /_temporary/0/_temporary/attempt_202305302140408743236050519850572_0111_m_000000_57 (exists=false, cwd=file:/home/mjoo/Bureau/Séminaire Big Data IPSL/Séminaire Big Data IPSL 2023/apache_spark_data_processing)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/30 21:40:40 WARN TaskSetManager: Lost task 0.0 in stage 111.0 (TID 57) (10.0.2.15 executor driver): java.io.IOException: Mkdirs failed to create file:/datasets/fireServiceParquet.parquet /_temporary/0/_temporary/attempt_202305302140408743236050519850572_0111_m_000000_57 (exists=false, cwd=file:/home/mjoo/Bureau/Séminaire Big Data IPSL/Séminaire Big Data IPSL 2023/apache_spark_data_processing)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "23/05/30 21:40:40 ERROR TaskSetManager: Task 0 in stage 111.0 failed 1 times; aborting job\n",
      "23/05/30 21:40:40 ERROR FileFormatWriter: Aborting job ae4f7e78-a9c7-4f73-bcb6-3ea05c204b8e.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 111.0 failed 1 times, most recent failure: Lost task 0.0 in stage 111.0 (TID 57) (10.0.2.15 executor driver): java.io.IOException: Mkdirs failed to create file:/datasets/fireServiceParquet.parquet /_temporary/0/_temporary/attempt_202305302140408743236050519850572_0111_m_000000_57 (exists=false, cwd=file:/home/mjoo/Bureau/Séminaire Big Data IPSL/Séminaire Big Data IPSL 2023/apache_spark_data_processing)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat $line71.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)\n",
      "\tat $line71.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n",
      "\tat $line71.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n",
      "\tat $line71.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:36)\n",
      "\tat $line71.$read$$iw$$iw$$iw$$iw.<init>(<console>:38)\n",
      "\tat $line71.$read$$iw$$iw$$iw.<init>(<console>:40)\n",
      "\tat $line71.$read$$iw$$iw.<init>(<console>:42)\n",
      "\tat $line71.$read$$iw.<init>(<console>:44)\n",
      "\tat $line71.$read.<init>(<console>:46)\n",
      "\tat $line71.$read$.<init>(<console>:50)\n",
      "\tat $line71.$read$.<clinit>(<console>)\n",
      "\tat $line71.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line71.$eval$.$print(<console>:6)\n",
      "\tat $line71.$eval.$print(<console>)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/datasets/fireServiceParquet.parquet /_temporary/0/_temporary/attempt_202305302140408743236050519850572_0111_m_000000_57 (exists=false, cwd=file:/home/mjoo/Bureau/Séminaire Big Data IPSL/Séminaire Big Data IPSL 2023/apache_spark_data_processing)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted.",
      "  at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)",
      "  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)",
      "  ... 37 elided",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 111.0 failed 1 times, most recent failure: Lost task 0.0 in stage 111.0 (TID 57) (10.0.2.15 executor driver): java.io.IOException: Mkdirs failed to create file:/datasets/fireServiceParquet.parquet /_temporary/0/_temporary/attempt_202305302140408743236050519850572_0111_m_000000_57 (exists=false, cwd=file:/home/mjoo/Bureau/Séminaire Big Data IPSL/Séminaire Big Data IPSL 2023/apache_spark_data_processing)",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:155)",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)",
      "  ... 67 more",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/datasets/fireServiceParquet.parquet /_temporary/0/_temporary/attempt_202305302140408743236050519850572_0111_m_000000_57 (exists=false, cwd=file:/home/mjoo/Bureau/Séminaire Big Data IPSL/Séminaire Big Data IPSL 2023/apache_spark_data_processing)",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)",
      "  at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)",
      "  at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)",
      "  at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)",
      "  at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)",
      "  at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:155)",
      "  at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)",
      "  at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:136)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "activeDowDF.write.parquet(\"/datasets/fireServiceParquet.parquet \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Industrialiser et Deployer le code source dans le cluster Hadoop  \n",
    "En s'appuyant sur le code source https://drive.google.com/drive/folders/1DeciYELjrEoAWxahJf00rf1NHFPsiiYZ?usp=share_link, industrialiser ce notebook en code Spark et le déployer job en mode cluster dans HadoopVagrant en stockant le dataframe final dans Hive en format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Examen Rattrapage  Big Data ISI",
   "notebookOrigID": 2952678889524244,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
